from fastapi import FastAPI, File, UploadFile, HTTPException, Form
from fastapi.responses import FileResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import pandas as pd
import numpy as np
import joblib
import io
import os
from datetime import datetime
from typing import Dict, List, Optional
import zipfile
from pathlib import Path

app = FastAPI(
    title="ExoScan AI API",
    description="Exoplanet Detection API using XGBoost",
    version="1.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

models = {}
label_encoders = {}
selected_features = {}

MISSIONS = {
    'kepler': {
        'model_file': 'exoplanet_xgboost_model.pkl',
        'encoder_file': 'label_encoder.pkl',
        'features_file': 'selected_features.pkl'
    },
    'k2': {
        'model_file': 'k2_exoplanet_xgboost_model.pkl',
        'encoder_file': 'k2_label_encoder.pkl',
        'features_file': 'k2_selected_features.pkl'
    },
    'tess': {
        'model_file': 'tess_exoplanet_xgboost_model.pkl',
        'encoder_file': 'tess_label_encoder.pkl',
        'features_file': 'tess_selected_features.pkl'
    }
}

print("Loading XGBoost models...")
for mission, files in MISSIONS.items():
    try:
        models[mission] = joblib.load(files['model_file'])
        label_encoders[mission] = joblib.load(files['encoder_file'])
        selected_features[mission] = joblib.load(files['features_file'])
        print(f"âœ“ {mission.upper()} model loaded with {len(selected_features[mission])} features")
    except Exception as e:
        print(f"âš  Error loading {mission.upper()} model: {e}")
        models[mission] = None

def preprocess_data(df: pd.DataFrame, mission: str) -> pd.DataFrame:
    """Preprocess data for specific mission"""
    required_features = selected_features[mission]
    
    missing_features = [f for f in required_features if f not in df.columns]
    available_features = [f for f in required_features if f in df.columns]
    
    if missing_features:
        print(f"Missing {len(missing_features)} features: {missing_features[:5]}")
    
    df_processed = df[available_features].copy()
    
    for feature in missing_features:
        df_processed[feature] = np.nan
    
    df_processed = df_processed[required_features]
    
    for col in df_processed.columns:
        if df_processed[col].isnull().sum() > 0:
            median_val = df_processed[col].median()
            if pd.isna(median_val):
                df_processed[col].fillna(0, inplace=True)
            else:
                df_processed[col].fillna(median_val, inplace=True)
    
    return df_processed

def create_summary_report(results_df: pd.DataFrame, processing_time: float, mission: str) -> str:
    total = len(results_df)
    predictions_upper = results_df['PREDICTION'].str.upper()
    
    confirmed = (predictions_upper.str.contains('CONFIRM', na=False)).sum()
    candidate = (predictions_upper.str.contains('CANDIDATE', na=False)).sum()
    false_positive = (predictions_upper.str.contains('FALSE', na=False)).sum()
    
    conf_cols = [col for col in results_df.columns if 'CONFIDENCE' in col.upper()]
    
    report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘            EXOPLANET DETECTION SUMMARY REPORT              â•‘
â•‘                   Mission: {mission.upper():8s}                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Processing Time: {processing_time:.2f} seconds

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ðŸ“Š ANALYSIS RESULTS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Total Analyzed: {total}
Confirmed: {confirmed} ({confirmed/total*100:.1f}%)
Candidates: {candidate} ({candidate/total*100:.1f}%)
False Positives: {false_positive} ({false_positive/total*100:.1f}%)

Generated by ExoScan AI
"""
    return report

@app.get("/")
async def root():
    available = [m for m, model in models.items() if model is not None]
    return {
        "message": "ExoScan AI API",
        "status": "running",
        "available_missions": available,
        "version": "1.0.0"
    }

@app.post("/predict")
async def predict_exoplanets(
    file: UploadFile = File(...),
    mission: str = Form(default='kepler')
):
    mission = mission.lower()
    
    if mission not in MISSIONS:
        raise HTTPException(status_code=400, detail=f"Invalid mission: {mission}")
    
    if models[mission] is None:
        raise HTTPException(status_code=500, detail=f"{mission.upper()} model not loaded")
    
    if not file.filename.endswith('.csv'):
        raise HTTPException(status_code=400, detail="Only CSV files supported")
    
    try:
        start_time = datetime.now()
        contents = await file.read()
        
        df = pd.read_csv(io.StringIO(contents.decode('utf-8')), comment='#')
        print(f"Received {mission.upper()}: {df.shape}")
        
        df_original = df.copy()
        df_processed = preprocess_data(df, mission)
        
        predictions = models[mission].predict(df_processed)
        predictions_proba = models[mission].predict_proba(df_processed)
        
        results_df = df_original.copy()
        results_df['PREDICTION'] = [label_encoders[mission].classes_[p] for p in predictions]
        
        for i, class_name in enumerate(label_encoders[mission].classes_):
            clean_name = str(class_name).upper().replace(" ", "_")
            results_df[f'CONFIDENCE_{clean_name}'] = predictions_proba[:, i]
        
        total = len(results_df)
        predictions_upper = results_df['PREDICTION'].str.upper()
        
        confirmed = (predictions_upper.str.contains('CONFIRM', na=False)).sum()
        candidate = (predictions_upper.str.contains('CANDIDATE', na=False)).sum()
        false_positive = (predictions_upper.str.contains('FALSE', na=False)).sum()
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_dir = Path('results')
        output_dir.mkdir(exist_ok=True)
        
        full_results_path = output_dir / f'{mission}_full_results_{timestamp}.csv'
        results_df.to_csv(full_results_path, index=False)
        
        confirmed_df = results_df[predictions_upper.str.contains('CONFIRM', na=False)]
        confirmed_path = output_dir / f'{mission}_confirmed_{timestamp}.csv'
        confirmed_df.to_csv(confirmed_path, index=False)
        
        processing_time = (datetime.now() - start_time).total_seconds()
        summary_text = create_summary_report(results_df, processing_time, mission)
        summary_path = output_dir / f'{mission}_summary_{timestamp}.txt'
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write(summary_text)
        
        zip_path = output_dir / f'{mission}_results_{timestamp}.zip'
        with zipfile.ZipFile(zip_path, 'w') as zipf:
            zipf.write(full_results_path, full_results_path.name)
            zipf.write(confirmed_path, confirmed_path.name)
            zipf.write(summary_path, summary_path.name)
        
        top_discoveries_list = []
        if len(confirmed_df) > 0:
            conf_cols = [col for col in confirmed_df.columns if 'CONFIDENCE' in col.upper() and 'CONFIRM' in col.upper()]
            if conf_cols and 'kepid' in confirmed_df.columns:
                for _, row in confirmed_df.nlargest(5, conf_cols[0]).iterrows():
                    top_discoveries_list.append({
                        'kepid': int(row['kepid']) if pd.notna(row.get('kepid')) else 0,
                        'confidence': float(row[conf_cols[0]])
                    })
        
        return {
            "success": True,
            "mission": mission,
            "summary": {
                "total_analyzed": int(total),
                "confirmed": int(confirmed),
                "candidates": int(candidate),
                "false_positives": int(false_positive),
                "confirmed_percentage": round(float(confirmed) / float(total) * 100, 1),
                "processing_time": round(processing_time, 2)
            },
            "top_discoveries": top_discoveries_list,
            "download_links": {
                "full_results": f"/download/{full_results_path.name}",
                "confirmed_only": f"/download/{confirmed_path.name}",
                "summary_report": f"/download/{summary_path.name}",
                "zip_all": f"/download/{zip_path.name}"
            }
        }
        
    except Exception as e:
        print(f"Error: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/download/{filename}")
async def download_file(filename: str):
    file_path = Path('results') / filename
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="File not found")
    return FileResponse(path=file_path, filename=filename)

if __name__ == "__main__":
    import uvicorn
    print("ðŸš€ ExoScan AI API - All missions loaded")
    uvicorn.run(app, host="0.0.0.0", port=8000)
